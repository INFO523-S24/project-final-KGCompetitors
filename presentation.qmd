---
title: "Personal Data Detection - KG Competitors"
subtitle: "INFO 523 - Spring 2024 - Project Final"
author: "Shashank Yadav<br>
         Gorantla Sai Laasya<br> 
         Maksim Kulik<br>
         Remi Hendershott<br>
         Surya Vardhan Dama<br>
         Kommareddy Monica Tejaswi<br>
         Priom Mahmud"
title-slide-attributes:
  data-background-image: images/4419038.jpg
  data-background-size: stretch
  data-background-opacity: "0.7"
  data-slide-number: none
format:
  revealjs:
    theme:  ['data/customtheming.scss']
    transition: concave
    background-transition: fade
    scrollable: true
    logo: images/icons8-competition-64.png
  
editor: visual
jupyter: python3
execute:
  echo: false
---

```{python}
#| label: load-packages
#| include: false

# Load packages here
import pandas as pd
import seaborn as sns
import plotly.graph_objects as go

```

```{python}
#| label: setup
#| include: false
#| 
# Set up plot theme and figure resolution
sns.set_theme(style="whitegrid")
sns.set_context("notebook", font_scale=1.1)

import matplotlib.pyplot as plt
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['figure.figsize'] = (6, 6 * 0.618)
```

```{python}
#| label: load-data
#| include: false
# Load data in Python
mtcars = sns.load_dataset('mpg').dropna()  # mtcars dataset is similar to the mpg dataset from seaborn
mtcars['speed'] = mtcars['horsepower'] / mtcars['weight']

penguins = sns.load_dataset('penguins').dropna()
```

## Introduction {.smaller}

-   Our team is participating in a Kaggle competition in which the primary goal is to develop a model capable of detecting personally identifiable information (PII) in student writing.
-   Our task focuses on word-level classification, where each word in a text belongs to one of several predefined categories, akin to a multiclass classification problem. This approach is commonly known as ‘Named Entity Recognition’. 
-   Our process involved creating and running three DeBERTa models of various sizes (extra small, small, and large) to solve the Kaggle objective, while being able to compare our three models and determine which one works best.


## Our Questions to Solve

-   Can we develop a model that successfully detects personally identifiable information (PII) in student writing?
-   How can we evaluate the model’s performance effectively? Which metrics are most appropriate for PII detection tasks?


## Dataset {.smaller}

-   Approximately 22,000 student essays, with each essay responding to a single assignment prompt, where each word in a text is assigned to one of 13 predefined labels. 
-   The key variables in the dataset include tokens representing simplified word forms, each associated with these labels.
-    Each label can be prefaced by either B (beginning) or I (inner), which is a way to tell whether a token is the first word of an entity or its continuation. The majority of tokens are labelled as “O”, (outer) meaning they do not constitute PII.
-   Preprocessing: The text is tokenized using DeBERTa’s tokenizer to ensure that named entities are correctly identified and segmented.



## Variables of the Dataset

```{python}
# Data for main plot
categories = ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 
           'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL']
           
all_data = [39, 78, 1365, 6, 2, 110, 6, 1, 1096, 15, 20, 1]
training_set = [36, 68, 1102, 4, 1, 82, 6, 1, 852, 12, 10, 1]
test_set = [3, 10, 263, 2, 1, 28, 0, 0, 244, 3, 10, 0]

# Create main plot
fig = go.Figure()

# Add bars for all categories
fig.add_trace(go.Bar(
    x=categories,
    y=all_data,
    name='All data',
    hovertemplate='%{x}: %{y}',
    marker_color='#074173'
))
fig.add_trace(go.Bar(
    x=categories,
    y=training_set,
    name='Training set',
    hovertemplate='%{x}: %{y}',
    marker_color='#75A47F'
))
fig.add_trace(go.Bar(
    x=categories,
    y=test_set,
    name='Test set',
    hovertemplate='%{x}: %{y}',
    marker_color='#FF8870'
))
# layout
fig.update_layout(
    title='Classwise distribution of Words ',
    xaxis_title='Categories',
    yaxis_title='Count (log scale)',
    barmode='group',
    yaxis_type='log'  # Setting the y-axis to a logarithmic scale
)
# figure
fig.show()
```

## Methodology {.smaller}

-   1. **Tokenization**: Tokenize the raw text using the appropriate DeBERTa tokenizer.
-   2. **Fine-Tuning**: Model are be fine-tuned on the same training set with a very small learning rate 6e-6. KerasNLP python package was used with JAX backed. Models were fine tuned on a Nvidia 4070Ti GPU.
-   3. **Evaluation**: Assessed each model's performance on the validation set F5-score. In the F5 score, recall is considered five times as important as precision. This means the F5 score is particularly useful in our case where missing out on positive cases (false negatives) is much more problematic than incorrectly labeling negative cases as positive (false positives) since there is a huge class imbalance.
-   4. **Comparison**: Compared the performance and resource utilization of each model.


## Model Summary {.smaller}

| Model Configuration       | Parameters (Millions)  | Training Details    | Expected Advantage     |
|----------------------------|:----------------------------:|--------------------------|-----------------------|
| DeBERTa-v3 Extra Small | 70.68 | - smaller batch size <br> - maximize model efficiency    | - lower computational requirements <br> - suitable for limited resources  |
| DeBERTa-v3 Small    | 141.30 | - moderate batch size <br> - balanced computational load and performance    | - better than extra small model with manageable resource use  |
| DeBERTa-v3 Large    | 434.01 | - larger batch size <br> - extended training periods    | - Highest accuracy and performance <br> -  suitable for resource-abundant scenarios |

* [DeBERTa-v3](https://huggingface.co/microsoft/deberta-v3-base)


## Results {.smaller}

-   The Extra Small DeBERTa model offers the fastest inference time but at the cost of lower F5 score.
-   The Small DeBERTa model provides a balanced performance with reasonable inference times and improved F5 Score over the extra small variant.
-   The Large DeBERTa model achieves the highest F5 score, reflecting its larger model capacity and outperformance.


## Results

![](images/violin.png){fig-align="center"}


## Conclusions {.smaller}

- Regarding our research goals and inquiries, we can indeed construct a model capable of accurately detecting personally identifiable information (PII) in student writings. To assess the efficiency of this model, we designed three different models of varying sizes and evaluated their performance to determine which was most effective. We compared their F5 scores and took their inference times into consideration. It became apparent that the large DeBERTa model achieved the highest F5 score, thereby emerging as the most effective model overall.
- However, the choice of model depends on the specific needs for efficiency and performance. For limited-resource environments, the Extra Small or Small models are recommended. For maximum accuracy, where resources are plentiful, the Large model is the best choice.

## References {.smaller}

- [1] Our Kaggle Competition Info can be found here: <https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/overview>
- For title page image: <https://www.freepik.com/free-vector/fingerprint-concept-illustration_10258681.htm#query=personal%20data&position=49&from_view=keyword&track=ais&uuid=a14f34fb-adf2-45bc-a8f5-ae2b66197593>
- Our logo: <https://icons8.com/icons/set/competition>
- Detect Fake Text: KerasNLP [TF/Torch/JAX][Train]
- Token classification
- Transformer ner baseline [lb 0.854]
- Thank You image: <https://www.freepik.com/free-vector/thank-you-concept-illustration_34680609.htm#page=2&query=thank%20you%20gif&position=29&from_view=keyword&track=ais&uuid=e51f94e5-1758-47eb-8cb9-8d69209b8ffd>

##
![](images/8188960.jpg){fig-align="center"}
